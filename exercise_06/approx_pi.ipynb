{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that was given in the exercise text. The goal of the exercise is to try to optimize it and to assess the impact with both timeit and cProfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_pi2(n=10000000):\n",
    "    val = 0.\n",
    "    for k in range(1,n+1):\n",
    "        val += 1./k**2\n",
    "    return (6 * val)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how I've optimized the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_pi2_opt(n=10000000):\n",
    "    val = sum([1./(k * k) for k in range(1, n+1)])\n",
    "    return (6 * val)**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the optimized calculation, as a string rather than as a function. It should run the same, but without the function call overhead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_pi2_string = \"(6*sum([1./(k * k) for k in range(1, 10000000+1)]))**.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a comparison of runtimes with timeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Function, Best Time: 6.786\n",
      "Optimized Function, Best Time: 2.680\n",
      "Optimized String, Best Time: 2.607\n",
      "Optimized Function, Speedup: 2.53\n",
      "Optimized String, Speedup: 2.60\n"
     ]
    }
   ],
   "source": [
    "num_tests = 1\n",
    "num_repeats = 20\n",
    "\n",
    "time_vector_original = timeit.repeat(\"approx_pi2()\", number = num_tests, repeat = num_repeats, globals=globals())\n",
    "time_best_original = min(time_vector_original)\n",
    "print(\"Original Function, Best Time: {:.3f}\".format(time_best_original))\n",
    "\n",
    "time_vector_opt = timeit.repeat(\"approx_pi2_opt()\", number = num_tests, repeat = num_repeats, globals=globals())\n",
    "time_best_opt = min(time_vector_opt)\n",
    "print(\"Optimized Function, Best Time: {:.3f}\".format(time_best_opt))\n",
    "\n",
    "time_vector_string = timeit.repeat(approx_pi2_string, number = num_tests, repeat = num_repeats, globals=globals())\n",
    "time_best_string = min(time_vector_string)\n",
    "print(\"Optimized String, Best Time: {:.3f}\".format(time_best_string))\n",
    "\n",
    "speedup_opt = time_best_original / time_best_opt\n",
    "speedup_string = time_best_original / time_best_string\n",
    "\n",
    "print(\"Optimized Function, Speedup: {:.2f}\".format(speedup_opt))\n",
    "print(\"Optimized String, Speedup: {:.2f}\".format(speedup_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like my optimized version runs about 2.5x as fast, which is cool. The string is slightly faster than the function, although I'm not sure that result would hold up to more rigorous testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the profile for the original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         4 function calls in 9.133 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    9.133    9.133    9.133    9.133 <ipython-input-2-ebb0c387764c>:1(approx_pi2)\n",
      "        1    0.000    0.000    9.133    9.133 <string>:1(<module>)\n",
      "        1    0.000    0.000    9.133    9.133 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run(\"approx_pi2()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't understand how I would use this to optimize the function. I can see that the entirety of the time takes place within the function, which makes sense since it's all I'm assessing. That doesn't give me any insight into how to make the function more efficient; I just did the optimization based on outside knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the profile for my optimized function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         6 function calls in 3.356 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.181    0.181    3.356    3.356 <ipython-input-3-fafa65d2d229>:1(approx_pi2_opt)\n",
      "        1    3.098    3.098    3.098    3.098 <ipython-input-3-fafa65d2d229>:2(<listcomp>)\n",
      "        1    0.000    0.000    3.356    3.356 <string>:1(<module>)\n",
      "        1    0.000    0.000    3.356    3.356 {built-in method builtins.exec}\n",
      "        1    0.077    0.077    0.077    0.077 {built-in method builtins.sum}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run(\"approx_pi2_opt()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one at least has a bit of a breakdown. Looking at the tottime, I can see that the function spends the most time on the list comprehension, which makes sense. I guess if I wanted to optimize further, that would be the area to target. \n",
    "\n",
    "Is the difference between the cumtime and the tottime for the function representative of the function call overhead? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         5 function calls in 3.034 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    2.693    2.693    2.693    2.693 <string>:1(<listcomp>)\n",
      "        1    0.232    0.232    3.034    3.034 <string>:1(<module>)\n",
      "        1    0.000    0.000    3.034    3.034 {built-in method builtins.exec}\n",
      "        1    0.109    0.109    0.109    0.109 {built-in method builtins.sum}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run(approx_pi2_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this one spends most of its time on the list comprehension. I'm noticing that this version spends more time on thestring module. What does that mean? It was negligible for the function versions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
